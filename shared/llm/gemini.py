"""Google Gemini provider for LLM functionality."""

import logging
from typing import Optional, Dict, Any

try:
    import google.generativeai as genai
    from google.generativeai.types import HarmCategory, HarmBlockThreshold
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

from .base import LLMProvider, LLMResponse, LLMError


logger = logging.getLogger(__name__)


class GeminiProvider(LLMProvider):
    """Google Gemini LLM provider."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize Gemini provider."""
        super().__init__(config)
        
        if not GEMINI_AVAILABLE:
            raise LLMError(
                "google-generativeai package not available. Install with: pip install google-generativeai",
                "gemini"
            )
        
        # Configure API key
        api_key = config.get("api_key")
        if not api_key:
            raise LLMError("Gemini API key not provided in configuration", "gemini")
        
        genai.configure(api_key=api_key)
        
        # Set up model configuration
        self.model_name = config.get("model", "gemini-1.5-flash")
        self.default_max_tokens = config.get("max_tokens", 2048)
        self.default_temperature = config.get("temperature", 0.7)
        
        # Safety settings - allow most content for flexibility
        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }
        
        logger.info(f"Initialized Gemini provider with model: {self.model_name}")
    
    async def generate(
        self, 
        prompt: str, 
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs
    ) -> LLMResponse:
        """Generate text using Gemini."""
        try:
            # Create model instance
            model = genai.GenerativeModel(
                model_name=self.model_name,
                system_instruction=system_prompt or self.get_default_system_prompt()
            )
            
            # Set generation config
            generation_config = genai.GenerationConfig(
                max_output_tokens=max_tokens or self.default_max_tokens,
                temperature=temperature or self.default_temperature,
                top_p=kwargs.get("top_p", 0.95),
                top_k=kwargs.get("top_k", 40),
            )
            
            # Generate response
            response = await model.generate_content_async(
                prompt,
                generation_config=generation_config,
                safety_settings=self.safety_settings
            )
            
            # Handle blocked responses
            if response.prompt_feedback and response.prompt_feedback.block_reason:
                raise LLMError(
                    f"Prompt was blocked: {response.prompt_feedback.block_reason}",
                    "gemini"
                )
            
            if not response.text:
                raise LLMError("No text generated by Gemini", "gemini")
            
            # Extract usage info if available
            usage = {}
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                usage = {
                    "prompt_tokens": getattr(response.usage_metadata, 'prompt_token_count', 0),
                    "completion_tokens": getattr(response.usage_metadata, 'candidates_token_count', 0),
                    "total_tokens": getattr(response.usage_metadata, 'total_token_count', 0)
                }
            
            return LLMResponse(
                text=response.text,
                model=self.model_name,
                provider="gemini",
                usage=usage,
                metadata={
                    "finish_reason": getattr(response.candidates[0], 'finish_reason', None) if response.candidates else None,
                    "safety_ratings": [
                        {
                            "category": rating.category.name,
                            "probability": rating.probability.name
                        }
                        for rating in (response.candidates[0].safety_ratings if response.candidates else [])
                    ]
                }
            )
            
        except Exception as e:
            if isinstance(e, LLMError):
                raise
            
            logger.error(f"Gemini generation error: {e}")
            raise LLMError(f"Gemini generation failed: {str(e)}", "gemini", e)
    
    async def health_check(self) -> bool:
        """Check if Gemini is accessible."""
        try:
            # Try a simple generation to test connectivity
            model = genai.GenerativeModel(model_name=self.model_name)
            response = await model.generate_content_async(
                "Hello",
                generation_config=genai.GenerationConfig(max_output_tokens=10)
            )
            return bool(response.text)
        except Exception as e:
            logger.warning(f"Gemini health check failed: {e}")
            return False
    
    def get_available_models(self) -> list[str]:
        """Get list of available Gemini models."""
        try:
            models = []
            for model in genai.list_models():
                if 'generateContent' in model.supported_generation_methods:
                    models.append(model.name.replace('models/', ''))
            return models
        except Exception as e:
            logger.warning(f"Could not list Gemini models: {e}")
            return ["gemini-1.5-flash", "gemini-1.5-pro", "gemini-1.0-pro"]