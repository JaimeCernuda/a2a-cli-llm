# Test configuration for A2A CLI with LLM providers
# This is a minimal config for testing - set your API keys via environment variables

default_provider: "ollama"  # Start with ollama since it's local

agent:
  name: "A2A CLI AI Test Agent"
  description: "Test agent with LLM capabilities"
  version: "0.2.0"
  system_prompt: |
    You are a helpful AI assistant in a test environment. Keep responses concise and friendly.
    When asked about yourself, mention you're an A2A protocol agent with LLM capabilities.

providers:
  # Ollama (local, no API key needed)
  ollama:
    base_url: "http://localhost:11434"
    model: "llama3.2:3b"
    max_tokens: 1024
    temperature: 0.7
    timeout: 30
    
  # Gemini (requires GEMINI_API_KEY environment variable)
  gemini:
    api_key: "${GEMINI_API_KEY}"
    model: "gemini-1.5-flash"
    max_tokens: 1024
    temperature: 0.7
    
  # Claude (requires CLAUDE_API_KEY environment variable)
  claude:
    api_key: "${CLAUDE_API_KEY}"
    model: "claude-3-5-haiku-20241022"
    max_tokens: 1024
    temperature: 0.7

server:
  host: "localhost"
  port: 8000
  log_level: "info"
  streaming: true

client:
  timeout: 30
  prefer_streaming: true

logging:
  level: "info"