"""Ollama provider for local LLM functionality."""

import logging
import httpx
import json
from typing import Optional, Dict, Any

from .base import LLMProvider, LLMResponse, LLMError


logger = logging.getLogger(__name__)


class OllamaProvider(LLMProvider):
    """Ollama local LLM provider."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize Ollama provider."""
        super().__init__(config)
        
        # Set up Ollama configuration
        self.base_url = config.get("base_url", "http://localhost:11434")
        self.model_name = config.get("model", "llama3.2:3b")
        self.default_max_tokens = config.get("max_tokens", 2048)
        self.default_temperature = config.get("temperature", 0.7)
        self.timeout = config.get("timeout", 60)
        
        logger.info(f"Initialized Ollama provider with model: {self.model_name}")
        logger.info(f"Ollama base URL: {self.base_url}")
    
    async def generate(
        self, 
        prompt: str, 
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs
    ) -> LLMResponse:
        """Generate text using Ollama."""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                # Prepare the request
                data = {
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": temperature or self.default_temperature,
                        "num_predict": max_tokens or self.default_max_tokens,
                    }
                }
                
                # Add system prompt if provided
                if system_prompt:
                    data["system"] = system_prompt
                else:
                    data["system"] = self.get_default_system_prompt()
                
                # Add additional options
                if "top_p" in kwargs:
                    data["options"]["top_p"] = kwargs["top_p"]
                if "top_k" in kwargs:
                    data["options"]["top_k"] = kwargs["top_k"]
                if "repeat_penalty" in kwargs:
                    data["options"]["repeat_penalty"] = kwargs["repeat_penalty"]
                
                # Make request to Ollama
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json=data,
                    headers={"Content-Type": "application/json"}
                )
                
                if response.status_code != 200:
                    raise LLMError(
                        f"Ollama API returned status {response.status_code}: {response.text}",
                        "ollama"
                    )
                
                result = response.json()
                
                if "error" in result:
                    raise LLMError(f"Ollama error: {result['error']}", "ollama")
                
                if not result.get("response"):
                    raise LLMError("No text generated by Ollama", "ollama")
                
                # Extract usage info if available
                usage = {}
                if "prompt_eval_count" in result:
                    usage["prompt_tokens"] = result.get("prompt_eval_count", 0)
                    usage["completion_tokens"] = result.get("eval_count", 0)
                    usage["total_tokens"] = usage["prompt_tokens"] + usage["completion_tokens"]
                
                return LLMResponse(
                    text=result["response"],
                    model=self.model_name,
                    provider="ollama",
                    usage=usage,
                    metadata={
                        "eval_duration": result.get("eval_duration"),
                        "load_duration": result.get("load_duration"),
                        "prompt_eval_duration": result.get("prompt_eval_duration"),
                        "total_duration": result.get("total_duration"),
                        "context": result.get("context", []),
                        "done": result.get("done", False)
                    }
                )
                
        except httpx.ConnectError as e:
            raise LLMError(
                f"Could not connect to Ollama at {self.base_url}. Make sure Ollama is running.",
                "ollama",
                e
            )
        except httpx.TimeoutException as e:
            raise LLMError(
                f"Ollama request timed out after {self.timeout} seconds",
                "ollama", 
                e
            )
        except Exception as e:
            if isinstance(e, LLMError):
                raise
            
            logger.error(f"Ollama generation error: {e}")
            raise LLMError(f"Ollama generation failed: {str(e)}", "ollama", e)
    
    async def health_check(self) -> bool:
        """Check if Ollama is accessible."""
        try:
            async with httpx.AsyncClient(timeout=10) as client:
                # First check if Ollama is running
                response = await client.get(f"{self.base_url}/api/tags")
                if response.status_code != 200:
                    return False
                
                # Check if our model is available
                models = response.json().get("models", [])
                model_names = [model["name"] for model in models]
                
                if self.model_name not in model_names:
                    logger.warning(f"Model {self.model_name} not found in Ollama. Available models: {model_names}")
                    return False
                
                # Try a simple generation
                test_response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model_name,
                        "prompt": "Hello",
                        "stream": False,
                        "options": {"num_predict": 5}
                    },
                    timeout=30
                )
                
                if test_response.status_code == 200:
                    result = test_response.json()
                    return bool(result.get("response"))
                
                return False
        except Exception as e:
            logger.warning(f"Ollama health check failed: {e}")
            return False
    
    async def get_available_models(self) -> list[str]:
        """Get list of available Ollama models."""
        try:
            async with httpx.AsyncClient(timeout=10) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                if response.status_code == 200:
                    models = response.json().get("models", [])
                    return [model["name"] for model in models]
                return []
        except Exception as e:
            logger.warning(f"Could not list Ollama models: {e}")
            return ["llama3.2:3b", "llama3.2:1b", "qwen2.5:3b", "phi3:mini"]
    
    async def pull_model(self, model_name: str) -> bool:
        """Pull a model from Ollama registry."""
        try:
            async with httpx.AsyncClient(timeout=300) as client:  # 5 minute timeout for downloads
                response = await client.post(
                    f"{self.base_url}/api/pull",
                    json={"name": model_name},
                    timeout=300
                )
                return response.status_code == 200
        except Exception as e:
            logger.error(f"Failed to pull Ollama model {model_name}: {e}")
            return False